# Operational Runbook - Blue/Green Deployment Alerts

This runbook provides guidance for responding to alerts generated by the Nginx monitoring system.

## Table of Contents

- [Alert Types](#alert-types)
  - [Failover Alert](#failover-alert)
  - [High Error Rate Alert](#high-error-rate-alert)
  - [Recovery Alert](#recovery-alert)
- [Alert Configuration](#alert-configuration)
- [Common Scenarios](#common-scenarios)
- [Troubleshooting](#troubleshooting)
- [Maintenance Mode](#maintenance-mode)

---

## Alert Types

### Failover Alert

**Alert Name:** `FAILOVER ALERT`  
**Color:** üü† Orange  
**Severity:** Medium

#### What It Means

Traffic has automatically failed over from one pool (Blue or Green) to the other. This happens when:
- The primary pool becomes unhealthy (health checks failing)
- The primary pool is returning 5xx errors
- The primary pool is timing out
- Nginx's circuit breaker has triggered

#### What You'll See

```
‚ö†Ô∏è FAILOVER ALERT
Traffic has failed over from blue pool to green pool.

Previous Pool: blue
Current Pool: green
Total Requests: 1234
Timestamp: 2025-10-30 14:23:45 UTC
```

#### Immediate Actions

1. **Acknowledge the alert** - Someone is aware and investigating

2. **Check the health of the failed pool:**
   ```bash
   # Check container health
   docker-compose ps
   
   # Check logs for errors
   docker-compose logs app_blue --tail=100
   
   # Test direct access
   curl -i http://localhost:8081/healthz
   curl -i http://localhost:8081/version
   ```

3. **Verify backup pool is serving correctly:**
   ```bash
   # Check that requests are successful
   for i in {1..5}; do
     curl -i http://localhost:8080/version | grep "X-App-Pool"
   done
   ```

4. **Check Nginx logs for details:**
   ```bash
   docker-compose exec nginx tail -50 /var/log/nginx/error.log
   docker-compose exec nginx tail -50 /var/log/nginx/monitoring.log
   ```

#### Investigation Steps

1. **Identify the root cause:**
   - Application crash or restart?
   - Memory/CPU exhaustion?
   - Network connectivity issue?
   - Database connection problem?
   - Recent deployment?

2. **Check application metrics:**
   ```bash
   # Check container resource usage
   docker stats app_blue app_green
   
   # Check application logs
   docker-compose logs app_blue --since=10m
   ```

3. **Review recent changes:**
   - Was there a recent deployment?
   - Did configuration change?
   - Are there known issues with this release?

#### Resolution

**If the primary pool is recoverable:**
1. Fix the underlying issue
2. Wait for health checks to pass (typically 5-15 seconds)
3. Monitor for recovery alert
4. Verify traffic returns to primary

**If the primary pool cannot be quickly recovered:**
1. Update `ACTIVE_POOL` in `.env` to make the backup the new primary
2. Restart services: `docker-compose up -d`
3. Plan remediation or rollback for the failed pool

#### Expected Outcome

- Backup pool handles 100% of traffic with zero client-visible errors
- Recovery alert when primary pool becomes healthy again
- If issue persists, consider rolling back or escalating

---

### High Error Rate Alert

**Alert Name:** `ERROR_RATE ALERT`  
**Color:** üî¥ Red  
**Severity:** High

#### What It Means

The upstream services are returning HTTP 5xx errors at a rate exceeding the configured threshold (default: 2% over the last 200 requests). This indicates:
- Application bugs causing crashes
- Database or external service failures
- Resource exhaustion (memory, CPU, connections)
- Configuration errors

#### What You'll See

```
‚ö†Ô∏è ERROR_RATE ALERT
Error rate has exceeded threshold: 5.50% (threshold: 2%)

Error Rate: 5.50%
Threshold: 2%
Window Size: 200 requests
Current Pool: green
Timestamp: 2025-10-30 14:25:12 UTC
```

#### Immediate Actions

1. **Acknowledge the alert**

2. **Check which pool is affected:**
   ```bash
   # Identify the current serving pool
   curl -i http://localhost:8080/version | grep "X-App-Pool"
   ```

3. **Review recent error logs:**
   ```bash
   # Check application errors
   docker-compose logs app_blue --tail=100 | grep -i error
   docker-compose logs app_green --tail=100 | grep -i error
   
   # Check Nginx upstream errors
   docker-compose exec nginx grep "upstream" /var/log/nginx/error.log | tail -20
   ```

4. **Check system resources:**
   ```bash
   # Check if containers are under resource pressure
   docker stats --no-stream
   
   # Check if containers are restarting
   docker-compose ps
   ```

#### Investigation Steps

1. **Identify the error pattern:**
   - Specific endpoints failing?
   - All requests failing or intermittent?
   - Started after a deployment?
   - Time-based pattern?

2. **Check application health:**
   ```bash
   # Test health endpoints
   curl http://localhost:8081/healthz  # Blue
   curl http://localhost:8082/healthz  # Green
   
   # Check version endpoints
   curl http://localhost:8081/version
   curl http://localhost:8082/version
   ```

3. **Review error details:**
   ```bash
   # Get detailed error logs
   docker-compose logs app_blue --since=5m > /tmp/blue_errors.log
   docker-compose logs app_green --since=5m > /tmp/green_errors.log
   
   # Look for stack traces, connection errors, timeouts
   grep -E "(Error|Exception|timeout)" /tmp/*.log
   ```

4. **Check dependencies:**
   - Database connectivity
   - External API availability
   - Network connectivity
   - DNS resolution

#### Resolution

**If error rate is below 50%:**
1. Monitor to see if it's transient
2. Check if failover to backup pool helps
3. Investigate root cause in parallel

**If error rate is above 50%:**
1. **URGENT:** Consider triggering manual failover:
   ```bash
   # If Blue is failing, trigger chaos to force failover to Green
   curl -X POST http://localhost:8081/chaos/start?mode=error
   ```

2. Roll back to last known good version:
   ```bash
   # Update .env with previous release
   BLUE_IMAGE=registry.example.com/app:previous-version
   docker-compose up -d app_blue
   ```

3. If both pools are affected:
   - Scale up resources if resource exhaustion
   - Fix underlying infrastructure issue
   - Enable maintenance mode if needed

#### Expected Outcome

- Error rate drops below threshold within 5 minutes
- Root cause identified and documented
- Preventive measures planned (if applicable)

---

### Recovery Alert

**Alert Name:** `RECOVERY ALERT`  
**Color:** üü¢ Green  
**Severity:** Informational

#### What It Means

The primary pool has recovered and is now serving traffic again. The system has automatically failed back from the backup pool to the primary pool.

#### What You'll See

```
‚ö†Ô∏è RECOVERY ALERT
Primary pool blue has recovered and is now serving traffic.

Pool: blue
Total Requests: 2456
Timestamp: 2025-10-30 14:28:30 UTC
```

#### Immediate Actions

1. **Verify recovery is stable:**
   ```bash
   # Send test traffic
   for i in {1..20}; do
     curl -i http://localhost:8080/version | grep "X-App-Pool"
     sleep 1
   done
   ```

2. **Monitor error rates:**
   ```bash
   # Watch logs for any errors
   docker-compose logs -f app_blue | grep -i error
   ```

3. **Check system metrics:**
   ```bash
   docker stats --no-stream app_blue
   ```

#### Investigation Steps

1. **Confirm issue is resolved:**
   - Review what caused the original failure
   - Verify the fix is in place
   - Ensure it won't recur

2. **Document the incident:**
   - Record timeline of events
   - Note root cause
   - List actions taken
   - Add lessons learned

#### Resolution

- Monitor for 15-30 minutes to ensure stability
- Update incident documentation
- No action needed if traffic remains stable

---

## Alert Configuration

### Environment Variables

Configure alerting behavior in `.env`:

```bash
# Slack webhook for alerts
SLACK_WEBHOOK_URL=https://hooks.slack.com/services/YOUR/WEBHOOK/URL

# Error rate threshold (percentage)
ERROR_RATE_THRESHOLD=2

# Window size (number of requests)
WINDOW_SIZE=200

# Alert cooldown (seconds)
ALERT_COOLDOWN_SEC=300

# Maintenance mode (true/false)
MAINTENANCE_MODE=false
```

### Tuning Recommendations

**Conservative (fewer false positives):**
```bash
ERROR_RATE_THRESHOLD=5
WINDOW_SIZE=500
ALERT_COOLDOWN_SEC=600
```

**Aggressive (catch issues faster):**
```bash
ERROR_RATE_THRESHOLD=1
WINDOW_SIZE=100
ALERT_COOLDOWN_SEC=120
```

---

## Common Scenarios

### Scenario 1: Planned Deployment

**Before deployment:**
```bash
# Enable maintenance mode to suppress alerts
sed -i 's/MAINTENANCE_MODE=false/MAINTENANCE_MODE=true/' .env
docker-compose up -d alert_watcher
```

**After deployment:**
```bash
# Disable maintenance mode
sed -i 's/MAINTENANCE_MODE=true/MAINTENANCE_MODE=false/' .env
docker-compose up -d alert_watcher
```

### Scenario 2: Both Pools Failing

If you receive error rate alerts for both pools:

1. **Check infrastructure:**
   ```bash
   # Database connectivity
   # External services
   # Network connectivity
   # DNS resolution
   ```

2. **Emergency rollback:**
   ```bash
   # Roll back both pools
   BLUE_IMAGE=registry/app:last-known-good
   GREEN_IMAGE=registry/app:last-known-good
   docker-compose up -d
   ```

### Scenario 3: Flapping (Rapid Failovers)

If you see multiple failover alerts in quick succession:

1. **Increase fail_timeout in nginx.conf:**
   ```nginx
   server app_blue:3000 max_fails=2 fail_timeout=30s;
   ```

2. **Check for resource contention:**
   ```bash
   docker stats
   ```

3. **Review health check reliability:**
   ```bash
   # Test health endpoint stability
   for i in {1..50}; do
     curl -w "%{http_code}\n" -s http://localhost:8081/healthz
     sleep 0.5
   done
   ```

---

## Troubleshooting

### No Alerts Received

1. **Check Slack webhook:**
   ```bash
   # Test webhook manually
   curl -X POST -H 'Content-type: application/json' \
     --data '{"text":"Test alert"}' \
     $SLACK_WEBHOOK_URL
   ```

2. **Check alert_watcher logs:**
   ```bash
   docker-compose logs alert_watcher
   ```

3. **Verify environment variables:**
   ```bash
   docker-compose exec alert_watcher env | grep SLACK
   ```

### False Positive Alerts

1. **Adjust thresholds:**
   - Increase `ERROR_RATE_THRESHOLD`
   - Increase `WINDOW_SIZE`
   - Increase `ALERT_COOLDOWN_SEC`

2. **Check log parsing:**
   ```bash
   # Verify log format
   docker-compose exec nginx tail -10 /var/log/nginx/monitoring.log
   ```

### Alert Storm (Too Many Alerts)

1. **Enable maintenance mode temporarily:**
   ```bash
   docker-compose exec alert_watcher sh -c 'export MAINTENANCE_MODE=true'
   ```

2. **Increase cooldown:**
   ```bash
   ALERT_COOLDOWN_SEC=900  # 15 minutes
   ```

---

## Maintenance Mode

### When to Use

- Planned deployments
- Blue/Green pool toggles
- Infrastructure maintenance
- Load testing

### How to Enable

**Option 1: Environment variable**
```bash
# In .env
MAINTENANCE_MODE=true

# Restart watcher
docker-compose up -d alert_watcher
```

**Option 2: Runtime (temporary)**
```bash
# Set for current watcher process
docker-compose exec alert_watcher sh -c 'export MAINTENANCE_MODE=true'
```

### What It Does

- Suppresses all alert types
- Logs continue to be monitored
- Statistics still tracked
- Alerts resume when disabled

### Remember to Disable

```bash
# In .env
MAINTENANCE_MODE=false

# Restart watcher
docker-compose up -d alert_watcher
```

---

## Quick Reference

### Essential Commands

```bash
# View all logs
docker-compose logs -f

# Check service status
docker-compose ps

# Test failover manually
curl -X POST http://localhost:8081/chaos/start?mode=error

# Stop chaos
curl -X POST http://localhost:8081/chaos/stop

# Check current serving pool
curl -i http://localhost:8080/version | grep "X-App-Pool"

# View alert watcher logs
docker-compose logs -f alert_watcher

# View Nginx monitoring logs
docker-compose exec nginx tail -f /var/log/nginx/monitoring.log
```

### Escalation Path

1. **Acknowledge alert** (0-2 minutes)
2. **Initial investigation** (2-5 minutes)
3. **Mitigation attempt** (5-15 minutes)
4. **Escalate if unresolved** (15+ minutes)

### Contact Information

- **On-call Engineer:** [Contact details]
- **Platform Team:** [Contact details]
- **Escalation:** [Contact details]

---

## Version History

| Version | Date | Changes |
|---------|------|---------|
| 1.0 | 2025-10-30 | Initial runbook creation |

---

**Last Updated:** 2025-10-30  
**Maintained By:** DevOps Team  
**Review Frequency:** Monthly
